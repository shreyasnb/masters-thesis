\chapter{Preliminaries} \label{ch:Preliminaries}

This chapter introduces mathematical concepts and notations related to optimization theory, behavioral approach to systems theory, and data-driven predictive control that will be used throughout this thesis.

\section{Geometry} \label{sec:opt}

In this section, we briefly review some fundamental concepts from Riemannian geometry and optimization on manifolds. For a more comprehensive treatment, the reader is referred to~\cite{absil,boumal2023}.

\subsection{Euclidean Spaces}
\begin{definition}[Inner Product]\label{def:euc-inner-prod}
    An inner product on a real vector space $\mathcal{E}$ is a function $\langle \cdot, \cdot \rangle : \mathcal{E} \times \mathcal{E} \to \R$ that satisfies the following properties for all $u,v,w \in \mathcal{E}$ and $a,b \in \R$:
    \begin{itemize}
        \item Symmetry: $\langle u, v \rangle = \langle v, u \rangle$,
        \item Linearity: $\langle au + bv, w \rangle = a\langle u, w \rangle + b\langle v, w \rangle$,
        \item Positive-definiteness: $\langle u, u \rangle \geq 0$ and $\langle u, u \rangle = 0 \iff u = 0$.
    \end{itemize}

\end{definition}

\begin{definition}[Euclidean Space]
    A linear space $\mathcal{E}$ equipped with an inner product $\langle \cdot, \cdot \rangle$ is called a Euclidean space. An inner product induces a norm on $\mathcal{E}$ called the Euclidean norm:
    \[
        \lVert u \rVert = \sqrt{\langle u, u \rangle}, \quad \forall u \in \mathcal{E}.
    \]
\end{definition}
The standard inner product on $\R^n$ and the associated norm are given by:
\begin{equation}\label{eq:std_inner}
    \langle u, v \rangle = u^\top v, \quad \lVert u \rVert_2 = \sqrt{u^\top u}, \quad \forall u,v \in \R^n.
\end{equation}
Similarly, the standard inner product on the space of real matrices $\R^{n \times k}$ is the Frobenius inner product, with the associated Frobenius norm:
\begin{equation}\label{eq:fro_inner}
    \langle A, B \rangle = \textrm{Tr}(A^\top B), \quad \lVert A \rVert_{\mathrm{F}} = \sqrt{\textrm{Tr}(A^\top A)}, \quad \forall A,B \in \R^{n \times k},
\end{equation}
where $\textrm{Tr}(M) = \sum_{i} M_{ii}$ denotes the trace of a matrix. We often use the following properties of the above inner product, with matrices $U, V, W, A, B$ of compatible sizes:
\begin{equation}
    \begin{split}
        \langle U, V  \rangle &= \langle U^\top, V^\top \rangle \\
        \langle A B, W \rangle &= \langle A, W B^\top \rangle = \langle B, A^\top W \rangle.
    \end{split}
\end{equation}

\begin{definition}[Differential]
    Let $U,V$ be two open sets in two Euclidean spaces $\mathcal{E}_1$ and $\mathcal{E}_2$, respectively. A map $F : U \to V$ is said to be \emph{smooth} if it is infinitely differentiable (class $\mathcal{C}^{\infty}$) on its domain. 

    Also, $F$ is said to be \emph{differentiable} at a point $x \in U$ if there exists a neighborhood $U'$of $x$ such that $F \mid_{U'}$ is smooth. Thus, if $F : U \to V$ is smooth at $x$, then its \emph{differential} at $x$, is the linear map $\textrm{D}F(x): \mathcal{E}_1 \to \mathcal{E}_2$ defined as:
    \[
        \textrm{D}F(x)[v] = \frac{d}{dt} F(x+tv) \Big|_{t = 0} =  \lim_{t \to 0} \frac{F(x + tv) - F(x)}{t}
    \]
\end{definition}

\begin{definition}[Euclidean Gradient]
    Consider a smooth function $f: \mathcal{E} \to \R$, where $\mathcal{E}$ is a linear space. The Euclidean gradient with respect to an inner product $\langle \cdot, \cdot \rangle: \mathcal{E} \times \mathcal{E} \to \R$, denoted by $\grad{f}(x)$ is a unique element of $\mathcal{E}$ such that, for all $v \in \mathcal{E}$, 
    \[
        \langle v, \grad{f}(x) \rangle = \textrm{D}f(x)[v],\quad x,v \in \mathcal{E},
    \]
    where $\textrm{D}f(x) : \mathcal{E} \to \R$ is the differential of $f$ at $x$.
\end{definition}

\begin{definition}[Euclidean Hessian]
    The Euclidean Hessian of a smooth function $f: \mathcal{E} \to \R$ is the linear map $\textrm{Hess}f(x) : \mathcal{E} \to \mathcal{E}$ defined as:
    \[
        \textrm{Hess}f(x)[v] = \textrm{D}(\grad{f})(x)[v] = \frac{d}{dt} \grad{f}(x + tv) \Big|_{t = 0}, \quad \forall x,v \in \mathcal{E}.
    \]
\end{definition}

\begin{definition}[Embedded Submanifold]
    Let $\mathcal{E}$ be a linear space of dimension $k$. A non-empty subset $\M$ of $\mathcal{E}$ is a (smooth) \emph{embedded submanifold} of $\mathcal{E}$ of dimension $n$ if either of the following conditions hold:
    \begin{enumerate}
        \item $n=k$ and $\M$ is open in $\mathcal{E}$, also called an open submanifold; or 
        \item $n=k-q$ for some $q \geq 1$ and, for each $x \in \M$, there exists an open neighborhood $U$ of $x$ in $\mathcal{E}$ and a smooth map $h : U \to \R^q$ such that:
        \begin{enumerate}[(a)]
            \item If $y$ is in $U$, then $h(y) = 0$ if and only if $y \in \M$; and
            \item \textrm{rank} $\textrm{D}h(x) = q$.
        \end{enumerate}
        Such a function $h$ is called a \emph{local defining function} for $\M$ at $x$.
    \end{enumerate}
    
\end{definition}

\begin{definition}[Tangent Space]
   Let $\M$ be a subset of $\mathcal{E}$. For all $x \in \M$, the \emph{tangent space} to $\M$ at $x$, denoted by $T_x \M$, is defined as the set of all vectors that are tangents at $x$ to smooth curves on $\M$, i.e.,
   \[
        T_x \M = \{ \gamma'(0) \mid \gamma : (-\epsilon, \epsilon) \to \M \textrm{ is a smooth curve with } \gamma(0) = x \}.
   \]
\end{definition}

\begin{definition}[Tangent Bundle]
    The \emph{tangent bundle} of a manifold $\M$ is the set of all tangent spaces over all points in $\M$:
    \[
        T\M = \{ (x,v) \mid x \in \M, v \in T_x \M \}.
    \]
\end{definition}

\subsection{Riemannian Geometry}
\begin{definition}[Differential]
    The differential of $F: \M \to \M'$ at the point $x \in \M$ is the linear map $DF(x) : T_x \M \to T_{F(x)} \M'$ defined as:
    \[
        \textrm{D}F(x)[v] = \frac{d}{dt} F(\gamma(t)) \Big|_{t = 0} = (F \circ \gamma)'(0),
    \]
    
    Let $\M$ and $\M'$ be two embedded submanifolds of two Euclidean spaces $\mathcal{E}$ and $\mathcal{E}'$, respectively. Then, the map $F : \M \to \M'$ admits a smooth extension $\bar{F} : U \to \mathcal{E}'$ defined on an open neighborhood $U$ of $\M$ in $\mathcal{E}$. Thus, for all $x \in \M$ and $v \in T_x \M$, the differential of $F$ at $x$ can be computed as:
    \[
        \textrm{D}F(x) = \textrm{D}\bar{F}(x) \mid_{T_x \M}.
    \]
\end{definition}

\begin{definition}[Riemannian Metric] \label{def:riem-metric}
    A metric $\langle \cdot, \cdot \rangle$ on $T_x \M$ is a Riemannian metric if it varies smoothly with $x$, in the sense that for all smooth vector fields $V,W$ on $\M$, the function $x \mapsto \langle V(x), W(x) \rangle_x$ is smooth.
\end{definition}

\begin{definition}[Riemannian Manifold] \label{def:riem-manifold}
    A Riemannian manifold is a smooth manifold $\M$ endowed with a \Nref*{def:riem-metric} $\langle \cdot, \cdot \rangle$.
    
\end{definition}

\begin{definition}[Riemannian Submanifold]
    An embedded submanifold $\M$ of a Euclidean space $\mathcal{E}$ is a Riemannian submanifold if it is endowed with the Riemannian metric induced by the inner product of $\mathcal{E}$, i.e., for all $x \in \M$ and $u,v \in T_x \M$,
    \[
        \langle u, v \rangle_x = \langle u, v \rangle,
    \]
    where $\langle \cdot, \cdot \rangle$ is the \Nref*{def:euc-inner-prod} on $\mathcal{E}$.
    
\end{definition}

\begin{definition}[Riemannian Gradient]
    Let $f : \mathcal{M} \to \R$ be smooth on a Riemannian manifold $\mathcal{M}$. The Riemannian gradient of $f$ is the vector field $\textrm{grad}f$ on $\mathcal{M}$ uniquely defined by the following identities:
    \[
        \forall (x,v) \in \textrm{T}\mathcal{M}, \quad \textrm{D}f(x)[v] = \langle v, \textrm{grad}f(x) \rangle_x,
    \]
    where $\textrm{D}f(x) : T_x \mathcal{M} \to \R$ is the differential of $f$ at $x$, and $\langle \cdot, \cdot \rangle_x$ is the Riemannian metric on $T_x \mathcal{M}$.
\end{definition}

\begin{definition}[Projection]
    Let $\M$ be an embedded submanifold of a Euclidean space $\mathcal{E}$ equipped with a Euclidean metric $\langle \cdot, \cdot \rangle$. The \emph{orthogonal projection} onto the tangent space $T_x \M$ at a point $x \in \M$ is the linear map $P^{\perp}_x : \mathcal{E} \to T_x \M$ which satisfies the following properties:
    \begin{enumerate}
        \item \emph{Range}: $\textrm{im}(P^{\perp}_x) = T_x \M$;
        \item \emph{Projector}: $P^{\perp}_x \circ P^{\perp}_x = P^{\perp}_x$;
        \item \emph{Orthogonal}: $\langle u - P^{\perp}_x(u), v \rangle = 0$, for all $u \in \mathcal{E}$ and $v \in T_x \M$.
    \end{enumerate}
    
\end{definition}

\begin{proposition}\label{prop:riem_grad}
    Let $\M$ be an embedded submanifold of a Euclidean space $\mathcal{E}$ equipped with a Euclidean metric $\langle \cdot, \cdot \rangle$. For a smooth function $f : \M \to \R$, the Riemannian gradient at a point $x \in \M$ is given by:
    \[
        \textrm{grad}f(x) = P^{\perp}_x(\grad{\bar{f}}(x)),
    \]
    where $\bar{f}$ is a smooth extension of $f$ to an open neighborhood of $\M$ in $\mathcal{E}$, and $\grad{\bar{f}}(x)$ is the Euclidean gradient of $\bar{f}$ at $x$.
\end{proposition}

\subsection{Grassmannians}
The Grassmannian $\Gr(k,n)$ is the set of all $k$-dimensional subspaces of $\R^n$. It can be endowed with the structure of a smooth \Nref*{def:riem-manifold} of dimension $k(n-k)$. Each point $\mathcal{S} \in \Gr(k,n)$ can be represented by an orthogonal matrix $Y \in \R^{n \times k}$ such that $Y^\top Y = I_k$.

The Grassmannian is a quotient manifold of the Stiefel manifold $\St(k,n)$, which is the set of all orthogonal $n \times k$ matrices. The equivalence class of $Y \in \St(k,n)$ is given by $[Y] = \{ Y Q \mid Q \in O(k) \}$, where $O(k)$ is the orthogonal group of $k \times k$ orthogonal matrices. Thus, $\Gr(k,n) \cong \St(k,n) / O(k)$ (refer \cite{boumal2023} for more details).

The projection operator onto the subspace $\mathcal{S}$ is given by $P_{\mathcal{S}} = Y Y^\top$ and the orthogonal complement projection is $P_{\mathcal{S}}^{\perp} = I_n - Y Y^\top$.

\begin{figure}[h]
    \centering
\begin{tikzpicture}[x={(1cm,0cm)}, y={(0.5cm,0.5cm)}, z={(0cm,1cm)}, scale=1, >=Stealth]

% Draw the subspace plane
\fill[blue!20, opacity=0.6] (0,-0.5,0) -- (3,-0.5,0) -- (3,1.5,0) -- (0,1.5,0) -- cycle;
% \draw[thick] (0,0,0) -- (3,0,0) -- (3,3,0) -- (0,3,0) -- cycle;
\node at (2.5,1,0) {$\mathcal{S}_{\hat{\mathbf{y}}}$};

% Draw coordinate axes
% \draw[->] (0,0,0) -- (4,0,0) node[below] {$x_1$};
% \draw[->] (0,0,0) -- (0,4,0) node[left] {$x_2$};
% \draw[->] (0,0,0) -- (0,0,4) node[left] {$x_3$};

% Draw vector x
\draw[->, thick, blue] (0,0,0) -- (2,1,3) node[midway, above left] {$x$};

% Draw projection w = P_S x
\draw[->, thick, red] (0,0,0) -- (2,1,0) node[midway, below right ] {$w = P_{\hat{\mathcal{S}}}x$};

% Draw perpendicular from x to w
\draw[dashed, purple] (2,1,3) -- (2,1,0) node[midway, right] {$x - w = P_{\hat{\mathcal{S}}}^\perp x$};

% Draw right angle indicator at w
\draw[thick, purple] (2,1,0.15) -- (1.85,1,0.12);
\draw[thick, purple] (1.85, 1,0.12) -- (1.85,1.01,-0.05);

% Origin label
% \node at (0,0,0) [below left] {$\mathbf{0}$};

\end{tikzpicture}
\caption{Depiction of the orthogonal projection of a vector onto a subspace $\hat{\mathcal{S}}$.}
\end{figure}

\begin{definition}[Chordal Distance]\label{defn:chordal}
    The chordal distance between two subspaces $\mathcal{S}_1, \mathcal{S}_2 \in \Gr(k,n)$ is defined as:
    \[
        d_{2}(\mathcal{S}_1, \mathcal{S}_2) = \sqrt{\mathrm{Tr}\left(P_{\mathcal{S}_1}^\perp P_{\mathcal{S}_2}\right)} = \frac{1}{\sqrt{2}}\lVert P_{\mathcal{S}_1} - P_{\mathcal{S}_2} \rVert_{\mathrm{F}} = \sqrt{\sum_{i=1}^{k} \sin^2(\theta_i)},
    \]
    where $\lVert \cdot \rVert_{\mathrm{F}}$ is the Frobenius norm, and $0 \leq \theta_1 \leq \cdots \leq \theta_k \leq \pi/2$ are the principal angles between $\mathcal{S}_1$ and $\mathcal{S}_2$.
    
\end{definition}

\begin{definition}[Gap Distance]
    The gap distance between two subspaces $\mathcal{S}_1, \mathcal{S}_2 \in \Gr(k,n)$ is defined as:
    \[
        d_{\infty}(\mathcal{S}_1, \mathcal{S}_2) = \lVert P_{\mathcal{S}_1} - P_{\mathcal{S}_2} \rVert_{2} = \sin(\theta_k),
    \]
    where $\lVert \cdot \rVert_{2}$ is the spectral norm, and $\theta_k$ is the largest principal angle between $\mathcal{S}_1$ and $\mathcal{S}_2$.
    
\end{definition}

\newpage
\section{Optimization}

\begin{definition}[Convexity]\label{def:conv}
    Let $\X$ be a real vector space. Let $f: \X \to \R$ be a function, such that $x \mapsto f(x)$. Then, $f$ is convex over $\X$, for all $x \in \X$, if and only if
    \[
        f(t x_1 + (1-t)x_2) \leq t f(x_1) + (1-t)f(x_2), 
    \]
    for all $0\leq t \leq 1$, and all $x_1, x_2 \in \X$.
\end{definition}
\begin{definition}[Constrained Optimization]
    For the constrained minimization problem,
    \begin{equation}\label{eq:cons-opt}
        \min_{x \in \R^n} f(x) \quad \textrm{subject to} \quad \begin{cases}
            c_i(x) = 0, \ i \in \mathcal{E}, \\
            c_i(x) \leq 0, \ i \in \mathcal{I},
        \end{cases}
    \end{equation}
\end{definition}
where $f$ and the functions $c_i$ are all smooth, real-valued functions on a subset of $\R^n$, and $\mathcal{I}$ and $\mathcal{E}$ are two finite sets of indices. We call $f$ the \emph{objective function}, while $c_i$, $i \in \mathcal{E}$ are the equality constraints and $c_i$ , $i \in \mathcal{I}$ are the inequality constraints. We define the \emph{feasible set} $\Omega$ to be the set of points $x$ that satisfy the constraints; that is,
\[
    \Omega = \{x \mid c_i(x) = 0, i \in \mathcal{E}; c_i(x) \leq 0, i \in \mathcal{I} \},
\]
so that we rewrite the problem more compactly as,
\[
    \min_{x \in \Omega} f(x).
\]
\begin{definition}[Lagrangian function]
    The \emph{Lagrangian} function for \eqref{eq:cons-opt} is defined as:
    \[
        \mathcal{L}(x, \lambda) = f(x) + \sum_{i \in \mathcal{I} \cap \mathcal{E}} \lambda_i c_i(x) 
    \]
\end{definition}
\begin{definition}[First-Order Optimality Conditions]
    Let $x^*$ be a local solution of \eqref{eq:cons-opt}, that the functions $f$ and $c_i$ in \eqref{eq:cons-opt} are continuously differentiable. Then there exists a Lagrange multiplier $\lambda^*$, with components $\lambda_i^*, i \in \mathcal{E} \cap \mathcal{I}$, such that the following conditions are satisfied at $(x^*, \lambda^*)$,
    \begin{subequations} \label{eq:kkt}
        \begin{alignat}{4}
            \nabla_x \mathcal{L}(x^*,\lambda^*) &= 0,  \\
            c_i(x^*) &= 0, \quad i \in \mathcal{E}, \\
            c_i(x^*) &\leq 0, \quad i \in \mathcal{I}, \\
            \lambda^* & \geq 0, \quad i \in \mathcal{I}, \\
            \lambda_i^* c_i(x^*) &= 0, \quad i \in \mathcal{E} \cap \mathcal{I}. \label{eq:kkt-comp}
        \end{alignat}
    \end{subequations}
These conditions \eqref{eq:kkt} are often known as the \emph{KKT (Karush-Kuhn-Tucker) conditions}. The conditions \eqref{eq:kkt-comp} are \emph{complementarity slackness conditions}; they imply that either constraint $i$ is active or $\lambda^*_i = 0$, or possibly both.
\end{definition}

\begin{definition}[Min-Max Optimization]
In general, the min-max problems (see \cite{han2023}, \cite{han2024}) on Riemannian manifolds that we focus on, are of the kind
\[
\min_{x \in \mathcal{M}_x} \max_{y \in \mathcal{M}_y} f(x,y),
\]
where $f$ is atleast $\mathcal{C}^2$ and $\M_x,\M_y$ are the Riemannian manifolds containing $x,y$ respectively. Without loss of generality, we assume that minimization takes place first, followed by maximization (cannot be interchanged). These problems are termed \emph{min-max optimization}, whereas the candidate solution points are termed \emph{minimax points}. 

\end{definition}
\begin{definition}[Global Minimax point]
\label{def:global-minimax}
    Let $\M_x, \M_y$ be two smooth Riemannian manifolds. Consider two subsets $\X, \Y$ such that $\X \times \Y \subseteq \M_x \times \M_y$. Let $f: \X \times \Y \to \R$, be a function such that $(x,y) \mapsto f(x,y)$. The point $(x^*, y^*)$ is called a global minimax point if for any $(x,y) \in \X \times \Y$, it satisfies:
    \[
    f(x^*,y) \leq f(x^*, y^*) \leq \max_{y' \in \Y} f(x, y').
    \]
\end{definition}
\begin{definition}[Local Minimax point]
\label{def:local-minimax}
A point $(x^*, y^*)$ is a local minimax point of $f: \X \times \Y \to \R$ if there exists some $\delta_0 > 0$, and a function $h$ such that $\lim_{\delta \to 0} h(\delta) = 0 \ \forall \ \delta \in (0, \delta_0]$ and for every $(x,y) \in \X \times \Y$ satisfying $d(x,x^*) \leq \delta$ and $d(y,y^*) \leq \delta$ such that
\[
f(x^*, y) \leq f(x^*, y^*) \leq \max_{y': d(y,y') \leq h(\delta)} f(x,y').
\]
\end{definition}

\subsection{Robust Least-Squares}
Consider the robust least-squares problem,
\begin{equation}\label{eq:robust-ls}
    \min_{x \in \R^n} \max_{\Sy \in \B^d_{\rho}(\hat{\Sy})} \lVert YY^\top x - b \rVert^2,
\end{equation}
where $\B^d_{\rho}(\hat{S}) = \{\Sy \in \Gr(k,n) \mid d(\Sy, \hat{\Sy}) \leq \rho \}$ is a ball on the Grassmannian centered at the subspace $\hat{\Sy}$ with radius $\rho > 0$, and $d(\cdot, \cdot)$ is a distance metric on the Grassmannian. For purposes of this discussion, we consider the chordal distance $d_2(\cdot, \cdot)$~(see \ref{defn:chordal} for definition).

\subsubsection{Convexity}
The robust least-squares problem in~\eqref{eq:robust-ls} is a non-concave in the inner maximization variable $\Sy$. However, it can be shown that the inner problem has an explicit solution, and hence as long as the outer minimization problem is convex, the overall problem remains tractable.
\begin{lemma}
    The robust least-squares problem in~\eqref{eq:robust-ls} is convex in the outer minimization variable $x$.
    \begin{proof}
        Let $f(x,Y) = \lVert YY^\top x - b \rVert^2$ denote the cost function in~\eqref{eq:robust-ls}. To show that $f$ is convex in $x$, we need to show that for any $x_1, x_2 \in \R^n$ and $0 \leq t \leq 1$, the following inequality holds (see~\ref{def:conv}):
        \[
            f(t x_1 + (1-t)x_2, Y) \leq t f(x_1, Y) + (1-t) f(x_2, Y).
        \]
        Expanding the left-hand side, we have:
        \begin{equation*}
        \begin{split}
            f(t x_1 + (1-t)x_2, Y) &= \lVert Y Y^\top (t x_1 + (1-t)x_2) - b \rVert^2 \\
            &= \lVert t (Y Y^\top x_1 - b) + (1-t)(Y Y^\top x_2 - b) \rVert^2.
        \end{split}
        \end{equation*}
        Using the convexity of the squared norm (using \emph{Jensen's inequality}), we have:
        \begin{equation*}
        \begin{split}
            f(t x_1 + (1-t)x_2, Y) &\leq t \lVert Y Y^\top x_1 - b \rVert^2 + (1-t) \lVert Y Y^\top x_2 - b \rVert^2 \\
            &= t f(x_1, Y) + (1-t) f(x_2, Y).
        \end{split}
        \end{equation*}
        Thus, $f(x,Y)$ is convex in $x$ for any fixed $Y$. Since the maximum of convex functions is also convex (\emph{preservation of convexity over maximization}), the overall problem in~\eqref{eq:robust-ls} is convex in $x$.
    \end{proof}
\end{lemma}


\begin{proposition}\label{prop:lag}
    If $(x^*, \Sy^*)$ is a local minimax point of the robust least-squares problem, then it is also a local minimax point of the following problem:
    \[
        \min_{x \in \R^n} \max_{\Sy \in \B^{d_2}_{\rho}(\hat{\Sy})} \mathrm{tr}\left(Y^\top B(x, \lambda) Y \right),
    \]
    where $B(x, \lambda) = A(x) + \lambda \hat{Y}\hat{Y}^\top$, and $A(x) = (x x^\top - b x^\top - x b^\top)$ for a \emph{KKT} multiplier $\lambda \geq 0$.
    \begin{proof}
        Let $f(x,Y) = \lVert YY^\top x - b \rVert^2$ denote the cost function in~\eqref{eq:robust-ls}. The ball constraint can be expressed as $g(Y) = d_2^2(Y, \hat{Y}) - \rho^2 \leq 0$. The Lagrangian function for the inner maximization problem is given by:
        \[
            \mathcal{L}(x,Y,\lambda) = f(x,Y) - \lambda g(Y),
        \]
        where $\lambda \geq 0$ is the \emph{KKT} multiplier (see~\eqref{eq:kkt} for details).
        Expanding~\eqref{eq:robust-ls}, and ignoring constants, we have:
        \begin{equation*}
        \begin{split}
            \mathcal{L}(x,Y,\lambda) &= \langle YY^\top x - b, YY^\top x - b \rangle - \lambda \left( \mathrm{tr}(Y^\top (\I - \hat{Y}\hat{Y}^\top) Y) - \rho^2 \right) \\
            &= \mathrm{tr}\left( Y^\top A(x) Y\right) - \lambda\left(k - \mathrm{tr}\left( Y^\top \hat{Y}\hat{Y}^\top Y \right) -\rho^2 \right) \\
            &= \mathrm{tr}\left( Y^\top \left(A(x) + \lambda \hat{Y}\hat{Y}^\top \right) Y \right) \\
            &= \mathrm{tr}\left( Y^\top B(x, \lambda) Y \right),
        \end{split}
        \end{equation*}
        where $A(x)$ and $B(x,\lambda)$ are defined as above.
    \end{proof}
\end{proposition}

\begin{lemma}\label{lem:top-k}
    Let $B \in \R^{n \times n}$ be a symmetric matrix. Define the function $h: \Gr(k,n) \to \R$ such that $h(\Sy) = \mathrm{tr}(Y^\top B Y)$, where $Y \in \R^{n \times k}$ is any orthogonal basis of the subspace $\Sy$. Then, $\max_{\Sy \in \Gr(k,n)} h(\Sy)$ has an explicit solution such that 
    \[
        \Sy^* = \{\textrm{top-k eigenspace of } B\}
    \]
    \begin{proof}
        Let the eigen-decomposition of $B$ be given by $B = U \Lambda U^\top$, where $\Lambda = \mathrm{diag}(\lambda_1, \ldots, \lambda_n)$ with $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$ being the eigenvalues of $B$, and $U = [u_1, u_2, \ldots, u_n]$ is the orthogonal matrix of corresponding eigenvectors. For any subspace $\Sy \in \Gr(k,n)$ with orthogonal basis $Y$, we can express $Y$ in terms of the eigenvectors of $B$ as $Y = U Q$, where $Q \in \R^{n \times k}$ satisfies $Q^\top Q = I_k$. Then,
        \begin{equation*}
        \begin{split}
            h(\Sy) &= \mathrm{tr}(Y^\top B Y) = \mathrm{tr}(Q^\top U^\top B U Q) = \mathrm{tr}(Q^\top \Lambda Q) \\
            &= \sum_{i=1}^{k} q_i^\top \Lambda q_i = \sum_{i=1}^{k} \sum_{j=1}^{n} \lambda_j {(q_{ij})}^2,
        \end{split}
        \end{equation*}
        where $q_i$ is the $i$-th column of $Q$. To maximize $h(\Sy)$, we need to choose $Q$ such that it maximizes the sum of the weighted eigenvalues. The maximum is achieved when the columns of $Q$ are chosen to be the top-$k$ eigenvectors of $B$. Thus, the optimal subspace $\Sy^*$ is spanned by the top-$k$ eigenvectors of $B$, i.e., 
        \[
            Y^* = \begin{bmatrix}
            | & | & & | \\
            | & | & & | \\
            u_1 & u_2 & \cdots & u_k \\
            | & | & & | \\
            | & | & & |
            \end{bmatrix}
        \]
        where $u_i$ is the $i$-th eigenvector of $B$. The optimal subspace is thus given by 
        \[
            \Sy^* = \mathrm{span}\{u_1, u_2, \ldots, u_k\}.
        \]
    \end{proof}
\end{lemma}

\begin{lemma}
    If $(x^*, \Sy^*)$ is a local minimax point of the robust least-squares problem~\eqref{eq:robust_lqt}, then the \emph{KKT} conditions for both the inner and outer problems are satisfied at $(x^*, \Sy^*)$ with some \emph{KKT} multiplier $\lambda^* \geq 0$.
    \begin{proof}
        Since $(x^*, \Sy^*)$ is a local minimax point of the robust least-squares problem~\eqref{eq:robust-ls}, it satisfies the first-order optimality conditions for both the inner maximization and outer minimization problems. Constructing the Lagrangian function for the inner maximization problem as in the previous proposition~\eqref{prop:lag}, we have:
        \[
            \mathcal{L}(x, \Sy, \lambda) = \mathrm{tr}\left(Y^\top B(x, \lambda) Y \right),
        \]
        where $B(x, \lambda) = A(x) + \lambda \hat{Y}\hat{Y}^\top$. The first-order optimality conditions for the inner maximization problem require that the gradient of the Lagrangian with respect to $\Sy$ vanishes at $\Sy^*$, i.e.,
        \begin{equation*}
            \begin{split}
                \textrm{grad}_{\Sy} \mathcal{L}(x^*, \Sy^*, \lambda^*) & = 0, \\
                P^{\perp}_{\Sy^*} \grad_Y \mathcal{\bar{L}}(x^*, Y^*, \lambda^*) & = 0, \\
                P^{\perp}_{\Sy^*}(2 B(x^*, \lambda^*) Y^*) & = 0, \\
                \implies B(x^*, \lambda^*) Y^* & = Y^* {Y^*}^\top B(x^*, \lambda^*) Y^*.
            \end{split}
        \end{equation*}
        Hence, the columns of $Y^*$ are eigenvectors of $B(x^*, \lambda^*)$, corresponding to the top-$k$ eigenvalues (from~\ref{lem:top-k}).

        Similarly, the first-order optimality conditions for the outer minimization problem require that the gradient of the Lagrangian with respect to $x$ vanishes at $x^*$, i.e.,
        \begin{equation*}
            \begin{split}
                \grad_x \mathcal{L}(x^*, \Sy^*, \lambda^*) & = 0 \\
                \grad_x \mathrm{tr}\left(Y^{* \top} B(x^*, \lambda^*) Y^* \right) & = 0 \\
                P_{\Sy^*}x^* &= P_{\Sy^*} b
            \end{split}
        \end{equation*}
    \end{proof}
    
\end{lemma}

\begin{lemma}\label{lem:boundary}
    If $\lambda > 0$, then the optimal subspace $\Sy^*$ for the inner problem lies on the boundary of the ball $\B_{\rho}(\hat{\Sy})$.
    \begin{proof}
        Assume, for the sake of contradiction, that the optimal subspace $\Sy^*$ lies in the interior of the ball $\B^{d_2}_{\rho}(\hat{\Sy})$. Then, we have $d_2(\Sy^*, \hat{\Sy}) < \rho$, which implies that $g(\Sy^*) < 0$. From the \emph{KKT} conditions (see~\eqref{eq:kkt}), we have the complementary slackness condition $\lambda g(\Sy^*) = 0$. Since $\lambda > 0$, it follows that $g(\Sy^*)$ must be equal to zero, which contradicts our assumption that $\Sy^*$ lies in the interior of the ball. Therefore, the optimal subspace $\Sy^*$ must lie on the boundary of the ball $\B^{d_2}_{\rho}(\hat{\Sy})$.
    \end{proof}
\end{lemma}

\begin{remark}\label{rem:lambda}
    Practically, the \emph{KKT} multiplier $\lambda$ can be interpreted as a regularization parameter that balances the trade-off between fitting the data and adhering to the constraint defined by the ball $\B^d_{\rho}(\hat{\Sy})$. A larger value of $\lambda$ places more emphasis on satisfying the constraint, while a smaller value allows for more flexibility in fitting the data. However, the choice of $\lambda$ value is not trivial and must be tuned based on satisfying the boundary condition in~\ref{lem:boundary}. This is typically implemented using techniques like line-search or bisection methods.
\end{remark}

\subsection{Constrained Robust Least-Squares}\label{subsec:cons-robust-ls}
From the perspective of application in predictive control, it is often desirable to impose a specific type of constraint, called the \emph{past constraint}. Thus, the constrained robust least-squares problem is formulated as:
\begin{equation}\label{eq:cons-robust-ls}
    \min_{x \in \R^n} \max_{\Sy \in \B^d_{\rho}(\hat{\Sy})} \lVert YY^\top x - b \rVert^2 \quad \textrm{subject to} \quad MYY^\top x = z,
\end{equation}
where $M \in \R^{d \times n}$ and $z \in \R^d$ define the equality constraint. The \emph{KKT} conditions for this problem can be derived similarly to the unconstrained case, with the addition of Lagrange multipliers for the equality constraints.

\begin{Note}
    Typically, $d < k$ and $M$ is a full row-rank matrix such that $M = [\I_d, 0]$ is a selection matrix. This constraint ensures that the first $d$ components of the projected vector $YY^\top x$ match the given vector $z$. This is particularly useful in scenarios such as in model predictive control applications where past states or inputs are fixed.
\end{Note}
\begin{remark}
    Although $M$ is always a full row-rank matrix, the matrix $C = MYY^\top$ may not be full row-rank for all orthogonal matrices $Y$. This is usually undesirable as it may lead to infeasibility of the equality constraint $Cx = z$. We will see how to address this issue using \emph{regularization} techniques. 
\end{remark}

\begin{proposition}
    The explicit solution $\Sy^*$ to the inner maximization problem in~\eqref{eq:cons-robust-ls} is given by
    \[
        \Sy^* = \{ \textrm{top-k eigenspace of } B_c(x, \lambda^*, \gamma^*)\}
    \]
    for some \emph{KKT} multipliers $\lambda, {\{\gamma_i\}}_{i=1}^d$, where
    \[
        B_c(x,\lambda,\gamma) = A(x) + \lambda \hat{Y}\hat{Y}^\top + M^\top \gamma x^\top
    \]
    \begin{proof}
        The problem~\eqref{eq:cons-robust-ls} can be posed as
        \begin{equation}
        \begin{split}
            \min_{x \in \R^n} & \max_{\Sy \in \Gr(k,n)} f(x,Y) \\
            \textrm{subject to} & \quad h(x,Y) = 0 \quad \textrm{and} \quad g(Y) \leq 0,
        \end{split}
        \end{equation}
        where the following functions are defined as:
        \begin{equation*}
            \begin{split}
                f(x,Y) &= \lVert YY^\top x - b \rVert^2 \\
                h(x,Y) &= MYY^\top x - z \\
                g(Y) &= d_2^2(Y,\hat{Y}) - \rho^2
            \end{split}
        \end{equation*}
        Constructing the Lagrangian function,
        \[
            \mathcal{\bar{L}}(x,Y,\lambda, \gamma) = f(x,Y) - \lambda g(Y) + \gamma^\top h(x,Y)
        \]
        which satisfies the \emph{KKT} stationarity condition 
        \[
            \textrm{grad}_{\Sy} \mathcal{L}(x^*,\Sy^*, \lambda^*, \gamma^*) = P^{\perp}_{\Sy^*} \grad_{Y} \mathcal{\bar{L}}(x^*, Y^*, \lambda^*, \gamma^*) = 0 
        \]
        Using Proposition~\ref{prop:lag}, we also have that
        \[
            \mathcal{\bar{L}}(x,Y, \lambda, \gamma) = \mathrm{tr}\left(Y^\top B_c(x,\lambda, \gamma) Y \right),
        \]
        where $B_c(x,\lambda,\gamma)$ is as proposed.

        Thus, the worst-case subspace is given by 
        \[
            \Sy^* = \{ \textrm{top-k eigenspace of } B_c(x, \lambda^*, \gamma^*)\}
        \]

    \end{proof}
\end{proposition}

\subsection{Regularization}\label{subsec:reg}
In practical implementations of the optimization problems discussed above, it is often beneficial to incorporate regularization terms to enhance numerical stability and ensure well-posedness. 

For example, in the above problems, we can easily observe that if $x^* \longrightarrow x^* + \eta$ for some $\eta \in \mathrm{Null}(YY^\top)$, then there are infinitely many optimal solutions that yield the same cost. To mitigate such issues, we can introduce a regularization term to the objective function. A common choice is to add $\ell_2$-norm regularization terms. This is a widely used technique under the name of \emph{Tikhonov regularization} (see~\cite{golub1999}) which has shown to improve the conditioning of least-squares problems.

\subsubsection{Norm Regularization}
Consider the regularized robust least-squares problem:
\begin{equation}\label{eq:reg-robust-ls}
    \min_{x \in \R^n} \max_{\Sy \in \B^d_{\rho}(\hat{\Sy})} \left(\lVert YY^\top x - b \rVert^2 + \mu \lVert x \rVert^2\right),
\end{equation}
where $\mu > 0$ is the regularization parameter. The addition of the term $\mu \lVert x \rVert^2$ helps to penalize large values of $x$, promoting solutions with smaller norms. This regularization can improve the conditioning of the optimization problem and lead to more stable solutions. 

We also observe that the problem becomes strongly convex in $x$ due to the presence of the $\ell_2$-norm regularization term. In fact, the Hessian of the objective function with respect to $x$ is given by:
\[
    \Hess_x f(x,Y) = 2 \left(YY^\top  +  \mu \I \right),
\]
which is positive definite for any $\mu > 0$. This strong convexity ensures the uniqueness of the optimal solution $x^*$.

\subsubsection{Constraint Regularization}\label{subsubsec:const-reg}
Consider the regularized robust least-squares problem:
\begin{equation}\label{eq:reg-cons-robust-ls}
    \min_{x \in \R^n} \max_{\Sy \in \B^d_{\rho}(\hat{\Sy})} \left( \lVert YY^\top x - b \rVert^2 + \gamma \lVert MYY^\top x - z \rVert^2 \right),
\end{equation}
where $\gamma > 0$ is a regularization parameter. The relaxation of the equality constraint to an inequality constraint allows for some flexibility in satisfying the constraint, which can be particularly useful in scenarios where exact satisfaction of the constraint may not be feasible due to noise or modeling errors. This regularization can help to ensure the feasibility of the optimization problem while still promoting solutions that are close to satisfying the original equality constraint.

This also helps in ensuring that the matrix $C = MYY^\top$ is full row-rank, thereby avoiding infeasibility issues. The equality constraint $h(x,Y) = 0$ is a coupled constraint involving both $x$ and $Y$, which affects convexity of the Lagrangian. 

\begin{algorithm}[H]\label{alg:cons-robust-ls}
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
        \KwData{Subspace Estimate $\hat{Y} \in \R^{n \times k}$, Vectors $b \in \R^n, z \in \R^d$}
        \Input{Initial guess $x_0 \in \R^n$, time-step $\alpha > 0$, $\lambda_0, \rho, \gamma > 0$, \texttt{tolx}}
        \Output{Optimal subspace $Y^*$, Optimal point $x^*$}
    \BlankLine
    \While{true}
    {
            $A_i = x_i x_i^\top - x_i b^\top - bx_i^\top$\;
            $Y = \texttt{eigs}(A_i,k)$\;
            \eIf{$d_2(Y,\hat{Y})\leq \rho$}{ 
                $\lambda^* = 0$\;
                $Y^* = Y$\;
            }{
            $\lambda^* = \arg \min_{\lambda} \big( d_2(Y(\lambda),\hat{Y}) - \rho \big)$\;
            $B_i = A_i + \lambda^* \hat{Y}\hat{Y}^\top$\;
            $Y^* = \{\texttt{top-k eigenvectors of } B_i \}$\;
            }   
            $v_i = \grad_{x_i}f(x_i, Y^*) = 2 Y^* {Y^*}^\top(x_i-b) + 2\gamma Y^*{Y^*}^\top M^\top (MY^*{Y^*}^\top x_i - z)$\;
        \eIf{$\lVert v_i \rVert \leq$ \texttt{tolx}}{
        break\;
        }{ 
        $x_{i+1} = x_i - \alpha v_i$; 
        }
    }
\caption{Constrained Robust Least-Squares Algorithm}
\end{algorithm}

\section{Systems Theory}
In this section, we briefly review some fundamental concepts in linear systems theory. Refer to~\cite{hespanha} for a comprehensive treatment.

\begin{definition}
[LTI System] \label{def:lti}
    A discrete-time linear time-invariant (LTI) system is described by the state-space equations:
    \[
        x(t+1) = A x(t) + B u(t), \quad y(t) = C x(t) + D u(t),
    \]
    where $x(t) \in \R^n$ is the state vector, $u(t) \in \R^m$ is the input vector, $y(t) \in \R^p$ is the output vector, and $A, B, C, D$ are constant matrices of appropriate dimensions.
\end{definition}

\begin{definition}
[Controllability]
    An \Nref*{def:lti} is said to be controllable if, for any initial state $x_0$ and any final state $x_f$, there exists a finite time $T$ and an input sequence $\{u(0), u(1), \ldots, u(T-1)\}$ that drives the state from $x_0$ to $x_f$ in $T$ steps.
\end{definition}

\begin{definition}
[Observability]
    An \Nref*{def:lti} is said to be observable if, for any initial state $x_0$, the output sequence $\{y(0), y(1), \ldots, y(T-1)\}$ over a finite time $T$ uniquely determines the initial state $x_0$.
\end{definition}  

\begin{definition}
[Minimal Realization]
    A realization of an \Nref*{def:lti} is said to be minimal if it has the smallest possible state dimension among all realizations that produce the same input-output behavior. A minimal realization is both controllable and observable.
\end{definition}

\begin{definition}
[Hankel Matrix]
    The Hankel matrix of an \Nref*{def:lti} is a structured matrix that captures the input-output behavior of the system. For a system with impulse response coefficients $\{h_0, h_1, h_2, \ldots\}$, the Hankel matrix $\mathcal{H}$ is defined as:
    \[
        \mathcal{H} = \begin{bmatrix}
        h_1 & h_2 & h_3 & \cdots \\
        h_2 & h_3 & h_4 & \cdots \\
        h_3 & h_4 & h_5 & \cdots \\
        \vdots & \vdots & \vdots & \ddots
        \end{bmatrix}.
    \]
\end{definition}

\begin{definition}[Kalman Rank Condition]
    An \Nref*{def:lti} is controllable if and only if the controllability matrix 
    \[
        \mathcal{C} = [B, AB, A^2B, \ldots, A^{n-1}B]
    \]
    has full row rank. Similarly, the system is observable if and only if the observability matrix
    \[
        \mathcal{O} = \begin{bmatrix}
        C \\
        CA \\
        CA^2 \\
        \vdots \\
        CA^{n-1}
        \end{bmatrix}
    \]
    has full column rank.
\end{definition}
